{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKV_SykKW4GO"
      },
      "source": [
        "# Privacy Policy Clustering Analysis\n",
        "This notebook demonstrates the complete machine learning pipeline from clustering privcy policies using graph transformed yml files. It includes data transformation, exploratory data analysis (EDA), clustering and evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOmTaWXPXlx1"
      },
      "source": [
        "# Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2Wg_sSpXg76"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "import matplotlib.cm as cm\n",
        "import seaborn as sns\n",
        "import yaml\n",
        "import os\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import umap\n",
        "from sklearn.manifold import TSNE\n",
        "import plotly.express as px\n",
        "from scipy.sparse import hstack\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWpMGG9YXtRC"
      },
      "source": [
        "##  Data Transformation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhFOXO0WXxo3"
      },
      "outputs": [],
      "source": [
        "def transform_graphs():\n",
        "    \"\"\"\n",
        "    Transforms privacy policy knowledge graphs from YAML files into a structured CSV format.\n",
        "\n",
        "    This function scans the local 'graphfiles/' directory for .yml files, extracts structured\n",
        "    information (such as source, target, relation type, purposes, and example texts) from each graph,\n",
        "    and compiles the results into a single CSV file named 'privacy_policy_data.csv'.\n",
        "\n",
        "    Each entry in the output CSV represents a data collection or processing relationship within a \n",
        "    privacy policy, suitable for downstream analysis like clustering or visualization.\n",
        "\n",
        "    The expected structure inside each YAML file includes:\n",
        "        - 'links': a list of dictionaries with keys like 'source', 'target', 'key', 'text', and 'purposes'.\n",
        "\n",
        "    Raises:\n",
        "        FileNotFoundError: If the 'graphfiles/' directory does not exist.\n",
        "\n",
        "    Output:\n",
        "        A CSV file named 'privacy_policy_data.csv' saved to the working directory.\n",
        "    \"\"\"\n",
        "\n",
        "    import os\n",
        "    import yaml\n",
        "    import pandas as pd\n",
        "\n",
        "    def extract_graph_data(filepath, app_name):\n",
        "        with open(filepath, 'r') as f:\n",
        "            graph = yaml.safe_load(f)\n",
        "        data = []\n",
        "        for link in graph.get(\"links\", []):\n",
        "            source = link.get(\"source\", \"\")\n",
        "            target = link.get(\"target\", \"\")\n",
        "            relation = link.get(\"key\", \"\")\n",
        "            texts = link.get(\"text\", [])\n",
        "            purpose_dict = link.get(\"purposes\", {})\n",
        "            categories = [k for k, v in purpose_dict.items() if v]\n",
        "            category_str = ', '.join(categories) if categories else 'unspecified'\n",
        "            purpose_list = [p for p_group in purpose_dict.values() for p in p_group] if purpose_dict else [\"unspecified\"]\n",
        "            purpose_str = ', '.join(purpose_list)\n",
        "            for text in texts:\n",
        "                data.append({\n",
        "                    \"App\": app_name,\n",
        "                    \"Source\": source,\n",
        "                    \"Relation\": relation,\n",
        "                    \"Target\": target,\n",
        "                    \"Example Text\": text,\n",
        "                    \"Purpose Categories\": category_str,\n",
        "                    \"Purpose Details\": purpose_str\n",
        "                })\n",
        "        return data\n",
        "\n",
        "    # Updated to look inside 'graph files/' folder\n",
        "    folder_path = \"./graph files\"  # relative path\n",
        "    # Validate folder exists\n",
        "    if not os.path.exists(folder_path):\n",
        "        raise FileNotFoundError(f\"Folder '{folder_path}' not found. Make sure it's in the same directory as this notebook.\")\n",
        "\n",
        "    file_paths = {\n",
        "        os.path.splitext(f)[0]: os.path.join(folder_path, f)\n",
        "        for f in os.listdir(folder_path)\n",
        "        if f.endswith(\".yml\")\n",
        "    }\n",
        "\n",
        "    combined_data = []\n",
        "    for app_name, filename in file_paths.items():\n",
        "        combined_data.extend(extract_graph_data(filename, app_name))\n",
        "\n",
        "    df = pd.DataFrame(combined_data)\n",
        "    df.to_csv(\"privacy_policy_data.csv\", index=False)\n",
        "    print(\" Saved as privacy_policy_data.csv\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47OfI6XSYARo"
      },
      "source": [
        "##  Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOEmRGNJYBvS"
      },
      "outputs": [],
      "source": [
        "\n",
        "def perform_eda():\n",
        "    \"\"\"\n",
        "    Performs exploratory data analysis (EDA) on the privacy policy dataset.\n",
        "\n",
        "    This function reads the generated 'privacy_policy_data.csv' file, analyzes\n",
        "    the distribution of key columns such as 'Relation', 'Purpose Categories', and 'Source',\n",
        "    and visualizes their frequencies using bar plots. It helps identify common data types\n",
        "    and relation patterns across different apps' privacy policies.\n",
        "\n",
        "    Assumes that 'privacy_policy_data.csv' has already been created by `transform_graphs()`.\n",
        "\n",
        "    Output:\n",
        "        Displays summary bar charts for:\n",
        "        - Purpose Categories distribution\n",
        "        - Relation types\n",
        "        - Top 10 most common Sources\n",
        "        - Top 10 most common Targets\n",
        "    \"\"\"\n",
        "\n",
        "    \n",
        "    df = pd.read_csv('privacy_policy_data.csv')\n",
        "    print(df.head())\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    df['Relation'].value_counts().plot(kind='barh', color='skyblue')\n",
        "    plt.title(\"Distribution of Relationship Types\")\n",
        "    plt.xlabel(\"Count\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    top_targets = df['Target'].value_counts().head(10)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    top_targets.plot(kind='barh', color='lightgreen')\n",
        "    plt.title(\"Top 10 Most Collected Data Types\")\n",
        "    plt.xlabel(\"Count\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    purpose_counts = df['Purpose Categories'].str.split(', ').explode().value_counts()\n",
        "    purpose_counts.plot.pie(autopct='%1.1f%%', startangle=140)\n",
        "    plt.title(\"Distribution of Data Use Purposes\")\n",
        "    plt.ylabel(\"\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    text_blob = ' '.join(df['Example Text'].dropna())\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text_blob)\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title(\"Word Cloud of Example Policy Text\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAYpKPrvScYe"
      },
      "source": [
        "## Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I-UvToTr-Ie4"
      },
      "outputs": [],
      "source": [
        "def perform_clustering(k=3, sample_size=None, svd_dim=50, tsne_dim=2, umap_dim=2):\n",
        "\n",
        "    \"\"\"\n",
        "    Performs clustering on privacy policy data using K-Means and visualizes results via dimensionality reduction.\n",
        "\n",
        "    This function:\n",
        "    - Loads the 'privacy_policy_data.csv' dataset.\n",
        "    - Vectorizes the 'Target' column using TF-IDF and encodes other columns using OneHotEncoder.\n",
        "    - Combines all features and applies K-Means clustering (with a user-defined k).\n",
        "    - Visualizes clustering results using SVD, t-SNE, and UMAP.\n",
        "    - Shows the elbow plot to help choose optimal k.\n",
        "    - Prints top TF-IDF keywords and metadata per cluster.\n",
        "\n",
        "    Args:\n",
        "        k (int): Number of clusters for K-Means (default is 3).\n",
        "        sample_size (int, optional): If set, samples the dataset to this number of rows.\n",
        "        svd_dim (int): Number of dimensions to retain before t-SNE/UMAP (default 50).\n",
        "        tsne_dim (int): Number of output dimensions for t-SNE (default 2).\n",
        "        umap_dim (int): Number of output dimensions for UMAP (default 2).\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing:\n",
        "            - 'df_clean': The cleaned and clustered DataFrame.\n",
        "            - 'combined_features': The vectorized feature matrix.\n",
        "            - 'clusters': Cluster labels from K-Means.\n",
        "            - 'svd': 2D coordinates from SVD.\n",
        "            - 'tsne': 2D coordinates from t-SNE.\n",
        "            - 'umap': 2D coordinates from UMAP.\n",
        "    \"\"\"\n",
        "\n",
        "    # Load data\n",
        "    df = pd.read_csv(\"privacy_policy_data.csv\")\n",
        "    df_clean = df[['Target', 'Purpose Categories', 'Relation', 'Source']].dropna()\n",
        "    df_clean['Target'] = df_clean['Target'].str.replace(' ', '_')\n",
        "\n",
        "    # Optional subsampling\n",
        "    if sample_size and sample_size < len(df_clean):\n",
        "        df_clean = df_clean.sample(sample_size, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    # Vectorization\n",
        "    tfidf = TfidfVectorizer()\n",
        "    tfidf_matrix = tfidf.fit_transform(df_clean['Target'])\n",
        "    encoder = OneHotEncoder(sparse_output=False)\n",
        "    encoded_other = encoder.fit_transform(df_clean[['Purpose Categories', 'Relation', 'Source']])\n",
        "    combined_features = hstack([tfidf_matrix, encoded_other])\n",
        "\n",
        "    # K-Means clustering\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "    clusters = kmeans.fit_predict(combined_features)\n",
        "    df_clean['Cluster'] = clusters\n",
        "    num_clusters = len(np.unique(clusters))\n",
        "    cluster_cmap = cm.get_cmap('rainbow', num_clusters)\n",
        "\n",
        "    # SVD Visualization\n",
        "    svd_vis = TruncatedSVD(n_components=2, random_state=42)\n",
        "    reduced_vis = svd_vis.fit_transform(combined_features)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(reduced_vis[:, 0], reduced_vis[:, 1], c=clusters, cmap=cluster_cmap, alpha=0.7)\n",
        "    for i in range(num_clusters):\n",
        "        plt.scatter([], [], c=[cluster_cmap(i)], label=f'Cluster {i}')\n",
        "    plt.title(\"SVD Clustering Visualization\")\n",
        "    plt.xlabel(\"SVD Component 1\")\n",
        "    plt.ylabel(\"SVD Component 2\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Elbow Method\n",
        "    inertia = []\n",
        "    k_values = list(range(1, 11))\n",
        "    for i in k_values:\n",
        "        km = KMeans(n_clusters=i, random_state=42)\n",
        "        km.fit(combined_features)\n",
        "        inertia.append(km.inertia_)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(k_values, inertia, marker='o')\n",
        "    plt.title(\"Elbow Method for Optimal k\")\n",
        "    plt.xlabel(\"Number of Clusters (k)\")\n",
        "    plt.ylabel(\"Inertia\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # SVD pre-reduction for t-SNE and UMAP\n",
        "    svd_model = TruncatedSVD(n_components=svd_dim, random_state=42)\n",
        "    reduced = svd_model.fit_transform(combined_features)\n",
        "\n",
        "    # t-SNE Visualization\n",
        "    tsne = TSNE(n_components=tsne_dim, random_state=42, perplexity=30)\n",
        "    tsne_features = tsne.fit_transform(reduced)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(tsne_features[:, 0], tsne_features[:, 1], c=clusters, cmap=cluster_cmap, alpha=0.7)\n",
        "    for i in range(num_clusters):\n",
        "        plt.scatter([], [], c=[cluster_cmap(i)], label=f'Cluster {i}')\n",
        "    plt.title(\"t-SNE Clustering Visualization\")\n",
        "    plt.xlabel(\"t-SNE Component 1\")\n",
        "    plt.ylabel(\"t-SNE Component 2\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # UMAP Visualization\n",
        "    umap_reducer = umap.UMAP(n_components=umap_dim, random_state=42)\n",
        "    umap_features = umap_reducer.fit_transform(reduced)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.scatter(umap_features[:, 0], umap_features[:, 1], c=clusters, cmap=cluster_cmap, alpha=0.7)\n",
        "    for i in range(num_clusters):\n",
        "        plt.scatter([], [], c=[cluster_cmap(i)], label=f'Cluster {i}')\n",
        "    plt.title(\"UMAP Clustering Visualization\")\n",
        "    plt.xlabel(\"UMAP Component 1\")\n",
        "    plt.ylabel(\"UMAP Component 2\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Cluster Summary\n",
        "    tfidf_feature_names = tfidf.get_feature_names_out()\n",
        "    for cluster_id in sorted(df_clean['Cluster'].unique()):\n",
        "        print(f\"\\n Cluster {cluster_id}:\")\n",
        "        indices = df_clean[df_clean['Cluster'] == cluster_id].index\n",
        "        cluster_tfidf = tfidf_matrix[indices].mean(axis=0).A1\n",
        "        top_indices = cluster_tfidf.argsort()[::-1][:5]\n",
        "        top_keywords = [tfidf_feature_names[i] for i in top_indices]\n",
        "        print(\"  Top Keywords (Target):\", top_keywords)\n",
        "        print(\"  Top Purpose Category:\", df_clean.loc[indices, 'Purpose Categories'].value_counts().idxmax())\n",
        "        print(\"  Top Relation:\", df_clean.loc[indices, 'Relation'].value_counts().idxmax())\n",
        "        print(\"  Top Source:\", df_clean.loc[indices, 'Source'].value_counts().idxmax())\n",
        "\n",
        "    return {\n",
        "        \"df_clean\": df_clean,\n",
        "        \"combined_features\": combined_features,\n",
        "        \"clusters\": clusters,\n",
        "        \"svd\": reduced_vis,\n",
        "        \"tsne\": tsne_features,\n",
        "        \"umap\": umap_features\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j81cpxSboPnp"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1dIGT8PoT-i"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.manifold import trustworthiness\n",
        "\n",
        "def eval(results):\n",
        "    \"\"\"\n",
        "    Evaluates the quality of clustering and dimensionality reduction.\n",
        "\n",
        "    Given the output from `perform_clustering()`, this function computes:\n",
        "    - Silhouette scores for each of the three embedding methods (SVD, t-SNE, UMAP),\n",
        "      which indicate how well-separated the clusters are.\n",
        "    - Trustworthiness scores to assess how well each dimensionality reduction\n",
        "      technique preserves local neighborhood structure from the original feature space.\n",
        "\n",
        "    Args:\n",
        "        results (dict): The dictionary returned by `perform_clustering()`, containing:\n",
        "            - 'combined_features': Original high-dimensional feature matrix.\n",
        "            - 'clusters': Cluster labels assigned by K-Means.\n",
        "            - 'svd', 'tsne', 'umap': 2D embedding coordinates for visualization.\n",
        "\n",
        "    Output:\n",
        "        Prints:\n",
        "        - Silhouette scores for each embedding method.\n",
        "        - Trustworthiness scores for each embedding method.\n",
        "    \"\"\"\n",
        "    \n",
        "    print(\"\\n=== Evaluation Metrics ===\")\n",
        "\n",
        "    # Extract required components\n",
        "    combined_features = results[\"combined_features\"]\n",
        "    clusters = results[\"clusters\"]\n",
        "    svd_vis = results[\"svd\"]\n",
        "    tsne_features = results[\"tsne\"]\n",
        "    umap_features = results[\"umap\"]\n",
        "\n",
        "    # Silhouette Scores\n",
        "    try:\n",
        "        svd_sil = silhouette_score(svd_vis, clusters)\n",
        "        tsne_sil = silhouette_score(tsne_features, clusters)\n",
        "        umap_sil = silhouette_score(umap_features, clusters)\n",
        "\n",
        "        print(\"Silhouette Scores:\")\n",
        "        print(f\"  SVD:   {svd_sil:.4f}\")\n",
        "        print(f\"  t-SNE: {tsne_sil:.4f}\")\n",
        "        print(f\"  UMAP:  {umap_sil:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(\"Silhouette Score error:\", e)\n",
        "\n",
        "    # Trustworthiness Scores\n",
        "    try:\n",
        "        svd_trust = trustworthiness(combined_features, svd_vis, n_neighbors=5)\n",
        "        tsne_trust = trustworthiness(combined_features, tsne_features, n_neighbors=5)\n",
        "        umap_trust = trustworthiness(combined_features, umap_features, n_neighbors=5)\n",
        "\n",
        "        print(\"\\nTrustworthiness Scores:\")\n",
        "        print(f\"  SVD:   {svd_trust:.4f}\")\n",
        "        print(f\"  t-SNE: {tsne_trust:.4f}\")\n",
        "        print(f\"  UMAP:  {umap_trust:.4f}\")\n",
        "    except Exception as e:\n",
        "        print(\"Trustworthiness Score error:\", e)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPIgpcwMYNTa"
      },
      "source": [
        "##  Main Pipeline Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zU4IbVSgYN97",
        "outputId": "84dfa0f0-41ad-49f6-ef1c-cf4fa9aeb91b"
      },
      "outputs": [],
      "source": [
        "\n",
        "def main():\n",
        "    print(\"Step 1: Transforming Graph YAML Files...\")\n",
        "    transform_graphs()\n",
        "    print(\"\\nStep 2: Performing Exploratory Data Analysis (EDA)...\")\n",
        "    perform_eda()\n",
        "    print(\"\\n Step 3: Clustering...\")\n",
        "    results = perform_clustering(k=3)\n",
        "    print(\"\\n Step 4: Evaluation...\")\n",
        "    eval(results)\n",
        "\n",
        "# Run the full pipeline\n",
        "main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "4mwU3VtFXh3f",
        "47OfI6XSYARo"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "neural",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
